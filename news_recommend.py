# -*- coding: utf-8 -*-
"""news_recommend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KBcvteGwwfRzKDQam1sfLu9LNFD5mqTb
"""

#import os,sys
#from google.colab import drive
#drive.mount('/content/drive',force_remount=True)
#os.listdir()
#os.chdir('/content/drive/My Drive/')
#sys.path.append('/content/drive/My Drive/')

import numpy as np
import pandas as pd
import pickle

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.cluster import MiniBatchKMeans

X=pd.read_csv("result.csv")
X

X = X[pd.isna(X['headline'])==False]
X = X[pd.isna(X['description'])==False]

X.head(3)

X.info()

# tfidf calculation
text_content = X['description']
vector = TfidfVectorizer(max_df=0.3,         # drop words that occur in more than X percent of documents
                             #min_df=8,      # only use words that appear at least X times
                             stop_words='english', # remove stop words
                             lowercase=True, # Convert everything to lower case 
                             use_idf=True,   # Use idf
                             norm=u'l2',     # Normalization
                             smooth_idf=True # Prevents divide-by-zero errors
                            )
tfidf = vector.fit_transform(text_content)

# Request function : search the top_n articles from a request ( request = string)
def search(tfidf_matrix,model,reques, top_n = 5):
    request_transform = model.transform([reques])
    similarity = np.dot(request_transform,np.transpose(tfidf_matrix))
    x = np.array(similarity.toarray()[0])
    indices=np.argsort(x)[-5:][::-1]
    return indices

# Find similar : get the top_n articles similar to an article 
def find_similar(tfidf_matrix, index, top_n = 5):
    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()
    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]
    return [index for index in related_docs_indices][0:top_n]    

# Print the result
def print_result(request_content,indices,X):
    print('\nsearch : ' + request_content)
    print('\nBest Results :')
    for i in indices:
        print('id = {0:5d} - headline = {1}'.format(i,X['headline'].loc[i]),X['link'].loc[i])


reques = input("search here")

result = search(tfidf,vector, reques, top_n = 5)
print_result(reques,result,X)

#Save objects on filesystem
pickle.dump(X, open('X', 'wb')) 
pickle.dump(vector, open('vector', 'wb')) 
pickle.dump(tfidf, open('tfidf', 'wb'))

